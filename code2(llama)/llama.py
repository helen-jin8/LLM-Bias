# -*- coding: utf-8 -*-
"""llama.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wQhhLLOdSR4OU1fKPV5BKI6p4DwmBBJ3
"""

# !pip install llama-index-llms-openai
# !pip install llama-index-embeddings-openai
# !pip install llama-index-finetuning
# !pip install llama-index-readers-file

import json

from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import MetadataMode

!mkdir -p 'data/10k/'
!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'
!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'

TRAIN_FILES = ["./data/10k/lyft_2021.pdf"]
VAL_FILES = ["./data/10k/uber_2021.pdf"]

TRAIN_CORPUS_FPATH = "./data/train_corpus.json"
VAL_CORPUS_FPATH = "./data/val_corpus.json"

def load_corpus(files, verbose=False):
    if verbose:
        print(f"Loading files {files}")

    reader = SimpleDirectoryReader(input_files=files)
    docs = reader.load_data()
    if verbose:
        print(f"Loaded {len(docs)} docs")

    parser = SentenceSplitter()
    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)

    if verbose:
        print(f"Parsed {len(nodes)} nodes")

    return nodes

train_nodes = load_corpus(TRAIN_FILES, verbose=True)
val_nodes = load_corpus(VAL_FILES, verbose=True)

from llama_index.finetuning import generate_qa_embedding_pairs
from llama_index.core.evaluation import EmbeddingQAFinetuneDataset

import os
from getpass import getpass


# Prompt for the OpenAI API key using getpass
OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')

# Set the environment variable
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

from llama_index.llms.openai import OpenAI


train_dataset = generate_qa_embedding_pairs(
    llm=OpenAI(model="gpt-3.5-turbo"), nodes=train_nodes
)
val_dataset = generate_qa_embedding_pairs(
    llm=OpenAI(model="gpt-3.5-turbo"), nodes=val_nodes
)

train_dataset.save_json("train_dataset.json")
val_dataset.save_json("val_dataset.json")

# [Optional] Load

train_dataset = EmbeddingQAFinetuneDataset.from_json("train_dataset.json")
val_dataset = EmbeddingQAFinetuneDataset.from_json("val_dataset.json")

from llama_index.finetuning import SentenceTransformersFinetuneEngine

finetune_engine = SentenceTransformersFinetuneEngine(
    train_dataset,
    model_id="BAAI/bge-small-en",
    model_output_path="test_model",
    val_dataset=val_dataset,
)

finetune_engine.finetune() # this takes ages to run! consider using much smaller training to run

# !pip install llama-index-embeddings-huggingface

embed_model = finetune_engine.get_finetuned_model()
embed_model

# Example text to encode into embeddings
texts = ["This is a test sentence.", "Here's another sentence."]

# Generate embeddings using the fine-tuned model
embeddings = embed_model.encode(texts)

# embeddings now contains the vector representations of your texts
print(embeddings)

from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import VectorStoreIndex
from llama_index.core.schema import TextNode
from tqdm.notebook import tqdm
import pandas as pd

from sentence_transformers.evaluation import InformationRetrievalEvaluator
from sentence_transformers import SentenceTransformer
from pathlib import Path


def evaluate(
    dataset,
    embed_model,
    top_k=5,
    verbose=False,
):
    corpus = dataset.corpus
    queries = dataset.queries
    relevant_docs = dataset.relevant_docs

    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]
    index = VectorStoreIndex(
        nodes, embed_model=embed_model, show_progress=True
    )
    print(index)
    retriever = index.as_retriever(similarity_top_k=top_k)

    eval_results = []
    for query_id, query in tqdm(queries.items()):
        retrieved_nodes = retriever.retrieve(query)
        retrieved_ids = [node.node.node_id for node in retrieved_nodes]
        expected_id = relevant_docs[query_id][0]
        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc

        eval_result = {
            "is_hit": is_hit,
            "retrieved": retrieved_ids,
            "expected": expected_id,
            "query": query_id,
        }
        eval_results.append(eval_result)
    return eval_results

def evaluate_st(
    dataset,
    model_id,
    name,
):
    corpus = dataset.corpus
    queries = dataset.queries
    relevant_docs = dataset.relevant_docs

    evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs, name=name)
    model = SentenceTransformer(model_id)
    return evaluator(model, output_path="/content")

bge = "local:BAAI/bge-small-en"
bge_val_results = evaluate(val_dataset, bge)

df_bge = pd.DataFrame(bge_val_results)

hit_rate_bge = df_bge['is_hit'].mean()

hit_rate_bge



evaluate_st(val_dataset, "BAAI/bge-small-en", name='bge')

from sentence_transformers import SentenceTransformer
import numpy as np

def generate_embeddings(texts, model):
    """
    Generate embeddings for a list of texts using the provided model.

    Args:
        texts (List[str]): The texts to encode into embeddings.
        model (SentenceTransformer): The model to use for generating embeddings.

    Returns:
        np.ndarray: The generated embeddings as a numpy array.
    """
    embeddings = model.encode(texts, convert_to_tensor=False, show_progress_bar=True)
    return embeddings

# Example usage:
# Assuming you have a list of texts you want to generate embeddings for
texts = ["This is a sentence.", "Here's another one."]

# Load your model - this can be your fine-tuned model or any SentenceTransformer model
model = SentenceTransformer('path/to/your/model' or 'sentence-transformers model name')

# Generate embeddings
embeddings = generate_embeddings(texts, model)

# Now, embeddings is a numpy array containing the vector representations of your texts
print(embeddings)